Trainer:
  exp_group_name: DVS128Gesture
  exp_num: 1e-5->1e-6 b24 datasetv2
  GPU_ids: [0]
  lr: 1e-5
  min_lr: 1e-6
  weight_decay: 0.05
  epoch: 100
  accumulation_steps: 1 # gradient accumulation, if not 0, remember lr *= accumulation_steps
  print_freq: 1
  Precision: fp16 # CLIP model default is float16
  seed: 101 # 3407 42 114514

MODEL:
  Load_Path: "Path-to-DVS128Gesture-[98.96].bin" # TODO: Change to your directory
#  Load_Path: None
  BACKBONE:
    Name: "ViT-B/16"
    Path:
    # CLIP model default is float16
    PRE_ENCODING: "fp16"
    #Download path to PRE_trained_model
    PRE_trained_model: 'Path-to/pretrained_CLIP/ViT-B-16.pt' # TODO:Change to your directory
  EventEncoder:
    train_clip_backbone: True
    # feature output from the No.3 ViT block, -1 denotes output whithout the low-level ferature
    use_cross_frame_prompts: True
    use_event_modality_prompts: True
    num_event_modality_prompts: 16
    use_temporal_encoding: True
    Low_level_feature_idx: [1]
  TextEncoder:
    init_ctx: True
    CTX_INIT: "A series of photos recording human action for"
    # length of text prompts
    leranable_ctx: True
    N_CTX: 16
    use_event_bias_textual_prompts: False
  Conceptualizing_Alignment:
    num_concept: 1  # 32 16  8  4   2
    concept_dim: 512 # 16 32 64 128 256
    sample_num: 5

Dataset:
  Input_size: [224,224]
  Train:
    Path: "Path-to-/DVS128Gesture_sampled_train.txt" # TODO: Change to your directory
    Batch_size: 12
    Representation: 'gray_scale' # mlp_learned, frame, gray_scale, rgb
    Agumentation: False
  Val:
    Path: "Path-to-/DVS128Gesture_sampled_val.txt" # TODO: Change to your directory
    Batch_size: 1
    Representation: 'gray_scale' # mlp_learned, frame, gray_scale, rgb
    Agumentation: False
  Classnames: "Path-to-/DVS128Gesture.json" # TODO: Change to your directory

LossFunction:
  loss_sigma_threshold: 15000
  loss_cross_entroy: False
  loss_cross_entroy_ori: False
  loss_logsigma_sum: True
  loss_SmoothL1Loss_txt: True
  loss_SmoothL1Loss_ev: True




