# EventBind: Learning a Unified Representation to Bind Them All for Event-based Open-world Understanding

This repository contains the official PyTorch implementation of the paper "[EventBind: Learning a Unified Representation to Bind Them All for Event-based Open-world Understanding](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=JQnsB8MAAAAJ&citation_for_view=JQnsB8MAAAAJ:zYLM7Y9cAGgC)" paper.
<div align="center">
<img src="image/EventBind.png" width="1300px">
</div>

**The codes and checkpoints will be released ASAP after the paper's decision.**

---
# Citation
If you find this paper useful, please consider staring üåü this repo and citing üìë our paper:

```
 @article{zhou2023clip,
  title={EventBind: Learning a Unified Representation to Bind Them All for Event-based Open-world Understanding},
  author={Zhou, Jiazhou and Zheng, Xu and Lyu, Yuanhuiyi and Wang, Lin},
  journal={arXiv preprint arXiv:2308.03135},
  year={2023}
}
```
---
# Dataset
<div align=center>

| Event Datasets |                                        Acesse to Download                                        | Corresponding Image Datasets | Acesse to Download |
|:--------------:|:------------------------------------------------------------------------------------------------:|:----------------------------:|:------------------:|
|  N-Caltech101  |       [Download](https://drive.google.com/drive/folders/1sY91hL_iHnmfRXSTc058bfZ0GQcEC6St)       |          Caltech101          |    [download](https://data.caltech.edu/records/mzrjq-6wc02)    |
|   N-Imagenet   | [Download](https://docs.google.com/document/d/1x0Vqe_5tVAJtYLYSZLwN6oNMExyUjIh-a30oLOKV2rE/edit) |           Imagenet           | [download](https://image-net.org/download.php)|
|    N-MINIST    | [Download](https://www.garrickorchard.com/datasets/n-mnist) |            MINIST            | [download](https://link.zhihu.com/?target=http%3A//yann.lecun.com/exdb/mnist/)|
</div>

---
# Dependencies
Please refer to [install.md](./docs/install.md) for step-by-step guidance on how to install the packages.

---
# Ô∏è Ô∏èAcknowledgement
We thank the authors of [CLIP](https://github.com/openai/CLIP), [CoOp](https://github.com/KaiyangZhou/Dassl.pytorch) for opening source their wonderful works.

---
# License
This repository is released under the [MIT](./LICENSE) License.

---
# Contact
If you have any question about this project, please feel free to contact jiazhouzhou@hkust-gz.edu.cn.
